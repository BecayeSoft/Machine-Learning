{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elementary concepts in Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXzbIvf8mDB7"
   },
   "source": [
    "## Spacy\n",
    "Spacy is an open-source Python library designed for natural language processing (NLP) tasks. It provides a wide range of NLP capabilities, including tokenization, part-of-speech tagging, named entity recognition, dependency parsing, and more. Spacy is known for its efficiency and ease of use, making it a popular choice for processing and analyzing textual data in various NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5623,
     "status": "ok",
     "timestamp": 1646046892849,
     "user": {
      "displayName": "Becaye Baldé",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhzgFcXv7DVgieb4tswKthnzT03m0raNDyR6mNDwQ=s64",
      "userId": "00603610922930475991"
     },
     "user_tz": -60
    },
    "id": "ucmYcttKmIh2",
    "outputId": "fbc1d2ad-cb5a-465a-9932-4ebe2d043775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlFEHXSgA6Bs"
   },
   "source": [
    "The text to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1646047197388,
     "user": {
      "displayName": "Becaye Baldé",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhzgFcXv7DVgieb4tswKthnzT03m0raNDyR6mNDwQ=s64",
      "userId": "00603610922930475991"
     },
     "user_tz": -60
    },
    "id": "ozlBxWGaA_nq"
   },
   "outputs": [],
   "source": [
    "my_sentence = \"Hello, I'm a french library\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5Cg0Cv3mKws"
   },
   "source": [
    "## Tokenization\n",
    "Tokenization is the process of breaking down a text or a sequence of characters into smaller units, known as tokens. These tokens can be individual words, sentences, or even subwords, depending on the level of granularity required for the analysis. Tokenization is a fundamental step in many natural language processing (NLP) tasks, as it provides a structured representation of textual data that can be further analyzed or processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1646046899863,
     "user": {
      "displayName": "Becaye Baldé",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhzgFcXv7DVgieb4tswKthnzT03m0raNDyR6mNDwQ=s64",
      "userId": "00603610922930475991"
     },
     "user_tz": -60
    },
    "id": "tcPyAMnLmRUU"
   },
   "outputs": [],
   "source": [
    "def return_token_text(sentence):\n",
    "    # Divide the sentence into tokens\n",
    "    doc = nlp(sentence)\n",
    "    # Return the text of the tokens\n",
    "    return [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1646046902804,
     "user": {
      "displayName": "Becaye Baldé",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhzgFcXv7DVgieb4tswKthnzT03m0raNDyR6mNDwQ=s64",
      "userId": "00603610922930475991"
     },
     "user_tz": -60
    },
    "id": "Vqh5bNkPBG4w",
    "outputId": "ea005a70-dbbf-499f-81fc-f7c4a9612b9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "['Hello', ',', 'I', \"'m\", 'a', 'french', 'library']\n"
     ]
    }
   ],
   "source": [
    "my_tok_text = return_token_text(my_sentence)\n",
    "print(\"Tokenizing...\")\n",
    "print(my_tok_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNS1KGRamSxw"
   },
   "source": [
    "## Stopwords\n",
    "Stopwords refer to commonly used words in a language that are often considered insignificant for analysis purposes. These words, such as \"the,\" \"is,\" \"and,\" \"in,\" etc., occur frequently in text but typically do not carry much meaning or contribute to the understanding of the context. In NLP, stopwords are often removed from text data before further analysis or processing, as they can add noise and hinder the performance of certain tasks, such as text classification or information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1646046905560,
     "user": {
      "displayName": "Becaye Baldé",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhzgFcXv7DVgieb4tswKthnzT03m0raNDyR6mNDwQ=s64",
      "userId": "00603610922930475991"
     },
     "user_tz": -60
    },
    "id": "ArM6yBU7mXDP",
    "outputId": "26ca73e7-033e-4a73-b352-579815414a62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopWords = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zY-2-8MbmmOH"
   },
   "source": [
    "### Ignoring the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1646046912585,
     "user": {
      "displayName": "Becaye Baldé",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhzgFcXv7DVgieb4tswKthnzT03m0raNDyR6mNDwQ=s64",
      "userId": "00603610922930475991"
     },
     "user_tz": -60
    },
    "id": "Kwi82SB6mqGA"
   },
   "outputs": [],
   "source": [
    "clean_words = []\n",
    "\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    for token in return_token_text(sentence):\n",
    "        if token not in stopWords:\n",
    "            clean_words.append(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1646046916781,
     "user": {
      "displayName": "Becaye Baldé",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhzgFcXv7DVgieb4tswKthnzT03m0raNDyR6mNDwQ=s64",
      "userId": "00603610922930475991"
     },
     "user_tz": -60
    },
    "id": "qmUzOVy3BZgX",
    "outputId": "dda05207-0568-4f25-a13a-28b9c185a1f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing stopwords...\n",
      "['Hello', ',', 'I', \"'m\", 'a', 'french', 'library']\n"
     ]
    }
   ],
   "source": [
    "remove_stopwords(my_sentence)\n",
    "print(\"Removing stopwords...\")\n",
    "print(clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XDiQBYFFpnH"
   },
   "source": [
    "# Stemming\n",
    "Stemming is a technique used in NLP to reduce words to their base or root form, known as a \"stem.\" It involves removing prefixes, suffixes, and inflectional endings from words to obtain the core morphological or semantic meaning. For example, stemming would convert the words \"running,\" \"runs,\" and \"ran\" to the common stem \"run.\" The purpose of stemming is to normalize words, reduce dimensionality, and improve text analysis tasks such as information retrieval, search engines, and text mining. However, it should be noted that stemming can sometimes result in non-dictionary words or stems that do not carry meaningful information, as it relies on simple linguistic rules and patterns.\n",
    "\n",
    "For example:\n",
    "* Going, Went, Gone are transformed into Go\n",
    "* Am, Are, Is -> Be\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfyEWA52B16-"
   },
   "source": [
    "### Snowball stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 353,
     "status": "ok",
     "timestamp": 1646046924640,
     "user": {
      "displayName": "Becaye Baldé",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhzgFcXv7DVgieb4tswKthnzT03m0raNDyR6mNDwQ=s64",
      "userId": "00603610922930475991"
     },
     "user_tz": -60
    },
    "id": "PBGXvp9hB3rS"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 365,
     "status": "ok",
     "timestamp": 1646047150869,
     "user": {
      "displayName": "Becaye Baldé",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhzgFcXv7DVgieb4tswKthnzT03m0raNDyR6mNDwQ=s64",
      "userId": "00603610922930475991"
     },
     "user_tz": -60
    },
    "id": "Sp6z-CUNBfRZ"
   },
   "outputs": [],
   "source": [
    "def return_stem(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [stemmer.stem(s.text) for s in doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 542,
     "status": "ok",
     "timestamp": 1646047201814,
     "user": {
      "displayName": "Becaye Baldé",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhzgFcXv7DVgieb4tswKthnzT03m0raNDyR6mNDwQ=s64",
      "userId": "00603610922930475991"
     },
     "user_tz": -60
    },
    "id": "vppXTYU1BhWW",
    "outputId": "da177399-fe7c-43fd-b7a0-c0e342a901b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming\n",
      "['hello', ',', 'i', \"'m\", 'the', 'french', 'librarian', 'gazing', 'a', 'the', 'star', 'abov', 'the', 'ringing']\n"
     ]
    }
   ],
   "source": [
    "my_stem = return_stem(my_sentence)\n",
    "print(\"Stemming\")\n",
    "print(my_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTHleYZRJOF6"
   },
   "source": [
    "### Porter Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 330,
     "status": "ok",
     "timestamp": 1646047980721,
     "user": {
      "displayName": "Becaye Baldé",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhzgFcXv7DVgieb4tswKthnzT03m0raNDyR6mNDwQ=s64",
      "userId": "00603610922930475991"
     },
     "user_tz": -60
    },
    "id": "3LTZKOVeFo1s",
    "outputId": "521cd124-b395-4322-c31d-dccf4e9501a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer\n",
      "cat\n",
      "troubl\n",
      "troubl\n",
      "troubl\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "print(porter.stem(\"trouble\"))\n",
    "print(porter.stem(\"troubling\"))\n",
    "print(porter.stem(\"troubled\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "es55hjMBJR6w"
   },
   "source": [
    "### Lancaster Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1646048566339,
     "user": {
      "displayName": "Becaye Baldé",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhzgFcXv7DVgieb4tswKthnzT03m0raNDyR6mNDwQ=s64",
      "userId": "00603610922930475991"
     },
     "user_tz": -60
    },
    "id": "UJNKvn57FvMo",
    "outputId": "2b2a17ab-eea2-433d-fd94-fbaa35bc0608",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancaster Stemmer\n",
      "My sentence: ['Daring', 'talking', 'to', 'my', 'darling', 'this', 'waying?']\n",
      "Stemming...\n",
      "My stem: ['dar', 'talk', 'to', 'my', 'darl', 'thi', 'waying?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "my_sentence = \"Daring talking to my darling this waying?\"\n",
    "my_stem = []\n",
    "\n",
    "my_sentence = my_sentence.split()\n",
    "for word in my_sentence:\n",
    "  stemmedWord = lancaster.stem(word)\n",
    "  my_stem.append(stemmedWord)\n",
    "\n",
    "print(\"My sentence:\", my_sentence)\n",
    "print(\"Stemming...\")\n",
    "print(\"My stem:\", my_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bag of Words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = ['Hello my name is is is Becaye',\n",
    "'Becaye this is my python notebook',\n",
    "'Becaye Becaye trying to create a big dataset',\n",
    "'Becaye of words to try different',\n",
    "'features of count vectorizer']\n",
    "\n",
    "coun_vect = CountVectorizer()\n",
    "count_matrix = coun_vect.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['becaye' 'big' 'count' 'create' 'dataset' 'different' 'features' 'hello'\n",
      " 'is' 'my' 'name' 'notebook' 'of' 'python' 'this' 'to' 'try' 'trying'\n",
      " 'vectorizer' 'words']\n"
     ]
    }
   ],
   "source": [
    "print(coun_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 8)\t3\n",
      "  (0, 0)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 11)\t1\n",
      "  (2, 0)\t2\n",
      "  (2, 17)\t1\n",
      "  (2, 15)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 15)\t1\n",
      "  (3, 12)\t1\n",
      "  (3, 19)\t1\n",
      "  (3, 16)\t1\n",
      "  (3, 5)\t1\n",
      "  (4, 12)\t1\n",
      "  (4, 6)\t1\n",
      "  (4, 2)\t1\n",
      "  (4, 18)\t1\n"
     ]
    }
   ],
   "source": [
    "print(count_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"bag of words.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to bag of words to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 1 3 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0]\n",
      " [2 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
      " [1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1]\n",
      " [0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "count_array = count_matrix.toarray()\n",
    "print(count_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now you are ready to dive into the Sentiment analysis notebook. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPhwxkiwcBiPtCPLdujhIBJ",
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
